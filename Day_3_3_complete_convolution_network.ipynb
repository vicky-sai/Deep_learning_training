{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day_3_3_complete_convolution_network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMUbETjOYIJIgYoVDRAR9Ly"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DBPttbOhAGFg","colab_type":"code","colab":{}},"source":["import numpy as np\n","from keras.datasets import cifar10\n","# TensorFlow and tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1x9CFu7EbJp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598551955270,"user_tz":-330,"elapsed":9632,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"f9907b45-c06a-46f4-8653-337dbcdd632d"},"source":["(X_train, y_train), (X_test, y_test) = cifar10.load_data()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BT-_3_OxAZ4B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598551956870,"user_tz":-330,"elapsed":740,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"3fb8fe6a-cb6e-4365-9155-d56a31f9296b"},"source":["print(type(X_train))\n","print(X_train.shape) # method to identify shape(size) of numpy.ndarray also known as a matrix"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","(50000, 32, 32, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EAIPN1QyAkCL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598551958338,"user_tz":-330,"elapsed":817,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"ba751e8f-0350-4d72-b9e8-f7a98f732571"},"source":["img1 = X_train[0, :, :, :] # : operator just means select all\n","print(img1.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(32, 32, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JuxyFpoTAoex","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598551960483,"user_tz":-330,"elapsed":1306,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"7bd0014a-cf8f-46fa-89d3-adb8cf86b6a1"},"source":["img1_again = X_train[0] # another way of selecting images\n","print(img1_again.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(32, 32, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y9aIfq-SArMF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598551963800,"user_tz":-330,"elapsed":1126,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"02a8c938-5a9a-47e3-dce4-506ac29686ab"},"source":["img_exp = X_train[0:30] # selection of multiple images can be easily done this way\n","print(img_exp.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(30, 32, 32, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n6o4HwcQAs_k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"status":"ok","timestamp":1598551965181,"user_tz":-330,"elapsed":1248,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"bcaa9e58-d917-46a3-d15d-ca054ef372e5"},"source":["import matplotlib.pyplot as plt\n","\n","plt.imshow(img1)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"uyoysYatAve1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598551966965,"user_tz":-330,"elapsed":1072,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"f228e984-0d2a-4050-b1c1-fa2573d9c437"},"source":["print(y_train.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(50000, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mLKLW2ZYBDQ-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1598551968606,"user_tz":-330,"elapsed":677,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"479025dd-8f2d-404c-c323-e128441a8117"},"source":["print(y_train[:5]) # here we look at the first 5 elements of y_train"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[6]\n"," [9]\n"," [9]\n"," [4]\n"," [1]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XxgmYPOABFM6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598551970733,"user_tz":-330,"elapsed":1156,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"53148078-2604-4ed3-8175-1b823574fa72"},"source":["print(y_train.min(), y_train.max())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qwmHh0I5BKDE","colab_type":"text"},"source":["As we expect, there are as many labels in y_train as images in X_train (50,000). Labels are integers range from 0 to 9 corresponding to the classes they represent. Here's a dictionary of what each integer represents."]},{"cell_type":"code","metadata":{"id":"VV96EqDcBG6b","colab_type":"code","colab":{}},"source":["labels = {\n","\t0: 'airplane',\n","  1: 'automobile',\n","  2: 'bird',\n","  3: 'cat',\n","  4: 'deer',\n","  5: 'dog',\n","  6: 'frog',\n","  7: 'horse',\n","  8: 'ship',\n","  9: 'truck'\n","} # remember img1 has label of 6, that corresponds to a frog"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bi_Qoqq0BOTz","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q11CsPRvBSMm","colab_type":"text"},"source":["Lastly lets check the size of our test set, I did mention above that CIFAR-10 has 60,000 labelled images and the training set has 50,000 images. So just to be sure"]},{"cell_type":"code","metadata":{"id":"f_qugn_VBTw6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598551986018,"user_tz":-330,"elapsed":1108,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"d81c47bf-dc4e-4398-8640-f0c911e99324"},"source":["print(X_train.shape)\n","print(y_train.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3)\n","(50000, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CqaPzd-3BcHV","colab_type":"text"},"source":["**Preprocessing**\n","\n","Preprocessing is an important step in building machine learning algorithms. There are things that you can do on both your X and y. Here we will explore 2 preprocessing techniques, mean-normalization and binary encoding.\n","\n","**Mean-normalization**\n","\n","Image pixel values are usually of the datatype uint8 which means an integer between the range of 0 to 255. If we make use of such large numbers in our models, there can be possibility of overflow (what happens when numbers get too big and the machine fails to compute correctly). To reduce possibility of overflow, we scale our original values down to a decimal between 0 and 1. Doing so is easy, we just have to divide every term by 255, the highest possible value."]},{"cell_type":"code","metadata":{"id":"DbPpC2DYBXgf","colab_type":"code","colab":{}},"source":["dtype_mult = 255.0\n","X_train = X_train.astype('float32') / dtype_mult"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C77kw-ASCmNl","colab_type":"text"},"source":["**Binary encoding**\n","\n","We want to be able to generate a probability index of how likely an image is to belong to each different class. Therefore we make a separate prediction for each class. To do that we also have to do a modification to our y as demonstrated below!"]},{"cell_type":"code","metadata":{"id":"3tdgdj9DCiRS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1598551994742,"user_tz":-330,"elapsed":1152,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"3ec8afc4-6ffa-4611-e975-04b021266d51"},"source":["import keras\n","\n","num_classes = 10\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","\n","print(y_train.shape)\n","\n","print(y_train[:5])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(50000, 10)\n","[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Znm_bu3SC3et","colab_type":"text"},"source":["As you can see, we basically transformed y_train into a binary code of is or is not. img1 which is labelled as a frog has an original label value of 6. From the single value of 6 it has transformed into an array of 10 digits, 0s everywhere except for the 6th place which has a value of 1. It just means that it is not a airplane, not a automobile ... but is a frog.\n","\n","**Model building**\n","\n","Now is time to define the model. before we declare the model, lets set out a clearly defined structure for our model before actually coding things out. We shall refer to the terminologies as defined in the explanation of CNNs.\n","\n","**Layer number**\n","CONV1 3x3\n","\n","RELU\n","\n","CONV2 3x3\n","\n","RELU\n","\n","POOL1 2x2\n","\n","CONV3 3x3\n","\n","RELU\n","\n","CONV4 3x3\n","\n","RELU\n","\n","POOL2 2x2\n","\n","FC1 256\n","\n","FC2 10 (as there as 10 classes)\n","\n","\n","Now that that's out of the way, here is how you define all this in code."]},{"cell_type":"code","metadata":{"id":"CqckId9xEQ-G","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Conv2D\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import MaxPooling2D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"whLbZN9-Cs_4","colab_type":"code","colab":{}},"source":["model = keras.Sequential() # our defined model functions in some sort of sequence, we use the Sequential class to initialize our model before adding the layers\n","\n","# Here's how you add layers to your model\n","# Conv1 32 32 (3) => 30 30 (32)\n","model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:])) # in layer 1 you need to specify input shape this is not needed in subsequent layers\n","model.add(Activation('relu'))\n","# Conv2 30 30 (32) => 28 28 (32)\n","model.add(Conv2D(32, (3, 3)))\n","model.add(Activation('relu'))\n","# Pool1 28 28 (32) => 14 14 (32)\n","model.add(MaxPooling2D(pool_size=(2, 2))) # the CONV CONV POOL structure is popularized in during ImageNet 2014\n","model.add(Dropout(0.25)) # this thing called dropout is used to prevent overfitting\n","\n","# Conv3 14 14 (32) => 12 12 (64)\n","model.add(Conv2D(64, (3, 3)))\n","model.add(Activation('relu'))\n","# Conv4 12 12 (64) => 6 6 (64)\n","model.add(Conv2D(64, (3, 3)))\n","model.add(Activation('relu'))\n","# Pool2 6 6 (64) => 3 3 (64)\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# FC layers 3 3 (64) => 576\n","model.add(Flatten()) # to turn input into a 1 dimensional array\n","# Dense1 576 => 256\n","model.add(Dense(256))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","# Dense2 256 => 10\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax')) # the softmax layer will scale all values down to between 0 and 1 which represents probability index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jbEqiVGiFznf","colab_type":"text"},"source":["The dropout layers works like this, choose a percentage of parameters randomly and discard them. Sounds counter intuitive but it works in ensuring that no parameter becomes overbearing on the entire model. Also it is a computationally cheap method to reduce overfitting. Do note that dropout layers do not activate during actual testing.\n","\n","Then you also have to define your parameter optimization strategy.\n","\n"]},{"cell_type":"code","metadata":{"id":"Fd-Vnnz2DrUO","colab_type":"code","colab":{}},"source":["optimizer = keras.optimizers.Adam() # Adam is one of many gradient descent formulas and one of the most popular"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZQU14OeF6Dn","colab_type":"text"},"source":["There are also other really good optimizers like RMSprop but for most cases Adam works well enough on it's own. You can attempt to change the learning rate and decay rate. But make sure you know how to conduct gradient descent before actually doing so!\n","\n","\n","Finally compile the model, simple as that.\n"]},{"cell_type":"code","metadata":{"id":"x8BexDnbF4Bo","colab_type":"code","colab":{}},"source":["model.compile(loss='categorical_crossentropy',\n","              optimizer=optimizer,\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzbX2lqCGD5F","colab_type":"text"},"source":["**Model training**\n","Using the dataset we can calculate the set of suitable parameters, the process of finding those parameters is called training. Training of model cannot be simpler."]},{"cell_type":"code","metadata":{"id":"_ZJuiIZnGAYX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1598553381181,"user_tz":-330,"elapsed":536088,"user":{"displayName":"Sudhanshu Saxena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtLNR6bSzgfK2Cto1rNC4C71j5LZTfq3vL5ZB7wT4=s64","userId":"06805949978483919255"}},"outputId":"452a4f7d-838c-4c15-978c-042653258874"},"source":["nb_epoch = 200 # number of iterations to train on\n","batch_size = 128 # process entire image set by chunks of 128\n","\n","model.fit(X_train, y_train, batch_size=128, epochs= 200) # be wawrned that the entire model can take over 4 hours to train if you are not using GPU"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.6171 - accuracy: 0.7799\n","Epoch 2/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.6033 - accuracy: 0.7853\n","Epoch 3/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5911 - accuracy: 0.7923\n","Epoch 4/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5781 - accuracy: 0.7948\n","Epoch 5/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5701 - accuracy: 0.7971\n","Epoch 6/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5545 - accuracy: 0.8052\n","Epoch 7/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5482 - accuracy: 0.8045\n","Epoch 8/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5342 - accuracy: 0.8110\n","Epoch 9/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5264 - accuracy: 0.8135\n","Epoch 10/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5249 - accuracy: 0.8126\n","Epoch 11/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5190 - accuracy: 0.8142\n","Epoch 12/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.5085 - accuracy: 0.8187\n","Epoch 13/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4986 - accuracy: 0.8222\n","Epoch 14/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4983 - accuracy: 0.8230\n","Epoch 15/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4808 - accuracy: 0.8286\n","Epoch 16/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4812 - accuracy: 0.8279\n","Epoch 17/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4810 - accuracy: 0.8301\n","Epoch 18/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4610 - accuracy: 0.8356\n","Epoch 19/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4639 - accuracy: 0.8337\n","Epoch 20/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4587 - accuracy: 0.8360\n","Epoch 21/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4493 - accuracy: 0.8389\n","Epoch 22/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4551 - accuracy: 0.8370\n","Epoch 23/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4388 - accuracy: 0.8423\n","Epoch 24/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4463 - accuracy: 0.8398\n","Epoch 25/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4357 - accuracy: 0.8443\n","Epoch 26/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4360 - accuracy: 0.8439\n","Epoch 27/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4292 - accuracy: 0.8457\n","Epoch 28/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4219 - accuracy: 0.8514\n","Epoch 29/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4267 - accuracy: 0.8480\n","Epoch 30/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4130 - accuracy: 0.8523\n","Epoch 31/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4134 - accuracy: 0.8524\n","Epoch 32/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4124 - accuracy: 0.8520\n","Epoch 33/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4102 - accuracy: 0.8525\n","Epoch 34/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4053 - accuracy: 0.8531\n","Epoch 35/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.4032 - accuracy: 0.8555\n","Epoch 36/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3954 - accuracy: 0.8591\n","Epoch 37/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3944 - accuracy: 0.8581\n","Epoch 38/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3931 - accuracy: 0.8613\n","Epoch 39/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3931 - accuracy: 0.8592\n","Epoch 40/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3907 - accuracy: 0.8598\n","Epoch 41/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3894 - accuracy: 0.8609\n","Epoch 42/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3870 - accuracy: 0.8632\n","Epoch 43/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3801 - accuracy: 0.8641\n","Epoch 44/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3809 - accuracy: 0.8636\n","Epoch 45/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3821 - accuracy: 0.8643\n","Epoch 46/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3789 - accuracy: 0.8646\n","Epoch 47/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3812 - accuracy: 0.8630\n","Epoch 48/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3780 - accuracy: 0.8637\n","Epoch 49/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3611 - accuracy: 0.8713\n","Epoch 50/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3706 - accuracy: 0.8679\n","Epoch 51/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3680 - accuracy: 0.8691\n","Epoch 52/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3651 - accuracy: 0.8694\n","Epoch 53/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3629 - accuracy: 0.8693\n","Epoch 54/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3605 - accuracy: 0.8713\n","Epoch 55/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3571 - accuracy: 0.8723\n","Epoch 56/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3613 - accuracy: 0.8695\n","Epoch 57/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3520 - accuracy: 0.8734\n","Epoch 58/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3545 - accuracy: 0.8740\n","Epoch 59/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3529 - accuracy: 0.8739\n","Epoch 60/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3573 - accuracy: 0.8727\n","Epoch 61/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3607 - accuracy: 0.8707\n","Epoch 62/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3452 - accuracy: 0.8755\n","Epoch 63/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3467 - accuracy: 0.8772\n","Epoch 64/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3476 - accuracy: 0.8772\n","Epoch 65/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3473 - accuracy: 0.8763\n","Epoch 66/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3469 - accuracy: 0.8768\n","Epoch 67/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3459 - accuracy: 0.8760\n","Epoch 68/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3446 - accuracy: 0.8755\n","Epoch 69/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3390 - accuracy: 0.8794\n","Epoch 70/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3405 - accuracy: 0.8785\n","Epoch 71/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3439 - accuracy: 0.8770\n","Epoch 72/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3432 - accuracy: 0.8780\n","Epoch 73/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3381 - accuracy: 0.8801\n","Epoch 74/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3388 - accuracy: 0.8789\n","Epoch 75/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3335 - accuracy: 0.8797\n","Epoch 76/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3350 - accuracy: 0.8809\n","Epoch 77/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3258 - accuracy: 0.8848\n","Epoch 78/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3342 - accuracy: 0.8791\n","Epoch 79/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3297 - accuracy: 0.8822\n","Epoch 80/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3270 - accuracy: 0.8827\n","Epoch 81/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3278 - accuracy: 0.8834\n","Epoch 82/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3255 - accuracy: 0.8836\n","Epoch 83/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3248 - accuracy: 0.8845\n","Epoch 84/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3247 - accuracy: 0.8842\n","Epoch 85/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3246 - accuracy: 0.8837\n","Epoch 86/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3256 - accuracy: 0.8841\n","Epoch 87/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3275 - accuracy: 0.8845\n","Epoch 88/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3274 - accuracy: 0.8827\n","Epoch 89/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3196 - accuracy: 0.8846\n","Epoch 90/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3216 - accuracy: 0.8852\n","Epoch 91/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3201 - accuracy: 0.8877\n","Epoch 92/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3108 - accuracy: 0.8887\n","Epoch 93/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3192 - accuracy: 0.8852\n","Epoch 94/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3194 - accuracy: 0.8867\n","Epoch 95/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3156 - accuracy: 0.8883\n","Epoch 96/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3193 - accuracy: 0.8874\n","Epoch 97/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3088 - accuracy: 0.8910\n","Epoch 98/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3141 - accuracy: 0.8886\n","Epoch 99/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3137 - accuracy: 0.8888\n","Epoch 100/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3088 - accuracy: 0.8904\n","Epoch 101/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3124 - accuracy: 0.8882\n","Epoch 102/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3162 - accuracy: 0.8870\n","Epoch 103/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3091 - accuracy: 0.8897\n","Epoch 104/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3073 - accuracy: 0.8916\n","Epoch 105/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3125 - accuracy: 0.8882\n","Epoch 106/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3054 - accuracy: 0.8923\n","Epoch 107/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3106 - accuracy: 0.8906\n","Epoch 108/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3104 - accuracy: 0.8897\n","Epoch 109/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3122 - accuracy: 0.8890\n","Epoch 110/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3057 - accuracy: 0.8919\n","Epoch 111/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3016 - accuracy: 0.8929\n","Epoch 112/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3103 - accuracy: 0.8915\n","Epoch 113/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3044 - accuracy: 0.8912\n","Epoch 114/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3042 - accuracy: 0.8922\n","Epoch 115/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3007 - accuracy: 0.8947\n","Epoch 116/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3023 - accuracy: 0.8914\n","Epoch 117/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3026 - accuracy: 0.8913\n","Epoch 118/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3021 - accuracy: 0.8927\n","Epoch 119/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2984 - accuracy: 0.8941\n","Epoch 120/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2926 - accuracy: 0.8953\n","Epoch 121/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2970 - accuracy: 0.8951\n","Epoch 122/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3001 - accuracy: 0.8948\n","Epoch 123/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3003 - accuracy: 0.8927\n","Epoch 124/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.3013 - accuracy: 0.8932\n","Epoch 125/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2935 - accuracy: 0.8958\n","Epoch 126/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2957 - accuracy: 0.8968\n","Epoch 127/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2978 - accuracy: 0.8943\n","Epoch 128/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2949 - accuracy: 0.8952\n","Epoch 129/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2918 - accuracy: 0.8982\n","Epoch 130/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2937 - accuracy: 0.8969\n","Epoch 131/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2931 - accuracy: 0.8962\n","Epoch 132/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2972 - accuracy: 0.8953\n","Epoch 133/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2917 - accuracy: 0.8988\n","Epoch 134/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2834 - accuracy: 0.9005\n","Epoch 135/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2865 - accuracy: 0.8984\n","Epoch 136/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2863 - accuracy: 0.8986\n","Epoch 137/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2918 - accuracy: 0.8970\n","Epoch 138/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2875 - accuracy: 0.8979\n","Epoch 139/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2940 - accuracy: 0.8957\n","Epoch 140/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2826 - accuracy: 0.9009\n","Epoch 141/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2926 - accuracy: 0.8974\n","Epoch 142/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2908 - accuracy: 0.8985\n","Epoch 143/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2902 - accuracy: 0.8968\n","Epoch 144/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2874 - accuracy: 0.8988\n","Epoch 145/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2836 - accuracy: 0.8993\n","Epoch 146/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2842 - accuracy: 0.8997\n","Epoch 147/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2846 - accuracy: 0.9000\n","Epoch 148/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2861 - accuracy: 0.8998\n","Epoch 149/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2837 - accuracy: 0.8986\n","Epoch 150/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2819 - accuracy: 0.8994\n","Epoch 151/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2788 - accuracy: 0.9009\n","Epoch 152/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2809 - accuracy: 0.9011\n","Epoch 153/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2836 - accuracy: 0.9001\n","Epoch 154/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2813 - accuracy: 0.9007\n","Epoch 155/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2845 - accuracy: 0.8992\n","Epoch 156/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2814 - accuracy: 0.9002\n","Epoch 157/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2843 - accuracy: 0.9013\n","Epoch 158/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2841 - accuracy: 0.9001\n","Epoch 159/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2769 - accuracy: 0.9018\n","Epoch 160/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2799 - accuracy: 0.9016\n","Epoch 161/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2739 - accuracy: 0.9021\n","Epoch 162/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2825 - accuracy: 0.9009\n","Epoch 163/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2804 - accuracy: 0.9017\n","Epoch 164/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2823 - accuracy: 0.9001\n","Epoch 165/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2797 - accuracy: 0.9015\n","Epoch 166/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2769 - accuracy: 0.9013\n","Epoch 167/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2725 - accuracy: 0.9031\n","Epoch 168/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2816 - accuracy: 0.9013\n","Epoch 169/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2797 - accuracy: 0.9028\n","Epoch 170/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2789 - accuracy: 0.9013\n","Epoch 171/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2744 - accuracy: 0.9039\n","Epoch 172/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2728 - accuracy: 0.9040\n","Epoch 173/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2716 - accuracy: 0.9041\n","Epoch 174/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2729 - accuracy: 0.9039\n","Epoch 175/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2763 - accuracy: 0.9036\n","Epoch 176/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2778 - accuracy: 0.9022\n","Epoch 177/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2828 - accuracy: 0.9013\n","Epoch 178/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2793 - accuracy: 0.9017\n","Epoch 179/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2694 - accuracy: 0.9057\n","Epoch 180/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2703 - accuracy: 0.9034\n","Epoch 181/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2777 - accuracy: 0.9022\n","Epoch 182/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2671 - accuracy: 0.9055\n","Epoch 183/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2711 - accuracy: 0.9042\n","Epoch 184/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2748 - accuracy: 0.9036\n","Epoch 185/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2724 - accuracy: 0.9033\n","Epoch 186/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2715 - accuracy: 0.9061\n","Epoch 187/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2746 - accuracy: 0.9032\n","Epoch 188/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2720 - accuracy: 0.9043\n","Epoch 189/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2762 - accuracy: 0.9032\n","Epoch 190/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2764 - accuracy: 0.9041\n","Epoch 191/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2729 - accuracy: 0.9040\n","Epoch 192/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2727 - accuracy: 0.9039\n","Epoch 193/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2697 - accuracy: 0.9054\n","Epoch 194/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2687 - accuracy: 0.9067\n","Epoch 195/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2715 - accuracy: 0.9044\n","Epoch 196/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2665 - accuracy: 0.9047\n","Epoch 197/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2684 - accuracy: 0.9067\n","Epoch 198/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2744 - accuracy: 0.9040\n","Epoch 199/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2660 - accuracy: 0.9064\n","Epoch 200/200\n","391/391 [==============================] - 3s 7ms/step - loss: 0.2603 - accuracy: 0.9077\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f3df021ba90>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"ZnPZB8gtLAKq","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}